# generated by fastapi-codegen:
#   filename:  openapi.json
#   timestamp: 2024-12-13T23:50:23+00:00

from __future__ import annotations

import logging
from collections.abc import Iterable
from dataclasses import asdict
from typing import Optional, Union

import httpx
from fastapi import FastAPI
from huggingface_hub import (
    ChatCompletionInput,
    ChatCompletionOutput,
    ChatCompletionStreamOutput,
    InferenceClient,
    ModelInfo,
)

from mtg_ai_webserver.models import (
    ChatRequest,
    ChatTokenizeResponse,
    CompatGenerateRequest,
    CompletionFinal,
    ErrorResponse,
    FieldDatamodelCodeGeneratorRootSpecialPostResponse,
    GenerateRequest,
    GenerateResponse,
    Info,
    ModelInfo,
    SagemakerRequest,
    SagemakerResponse,
    StreamResponse,
    TokenizeResponse,
)

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


_app_instance: Optional[FastAPI] = None

def get_application() -> FastAPI:
    global _app_instance
    if _app_instance is None:
        _app_instance = FastAPI(
            title="Text Generation Inference",
            description="Text Generation Webserver",
            contact={"name": "Thomas Meyer", "email": "thomas@thomasmeyer.co"},
            license={
                "name": "Apache 2.0",
                "url": "https://www.apache.org/licenses/LICENSE-2.0",
            },
            version="3.0.1-dev0",
        )
    return _app_instance

app = get_application()

inference_client = InferenceClient(model="http://text-generation-inference:8080")

web_client = httpx.AsyncClient(http2=True)

# @app.middleware("http")
# async def log_requests(request: Request, call_next: Callable[[Request], Awaitable[Response]]) -> Response:
#     logger.info(f"Middleware Rquest: {request.method} URL: {request.url} Body: {await request.json()}")
#     test_req = ChatRequest.model_validate(await request.json())
#     logger.info(test_req)
#     response = await call_next(request)
#     logger.info(f"Middleware Response: {response.status_code}")
#     return response


@app.post(
    "/",
    response_model=FieldDatamodelCodeGeneratorRootSpecialPostResponse,
    responses={
        "422": {"model": ErrorResponse},
        "424": {"model": ErrorResponse},
        "429": {"model": ErrorResponse},
        "500": {"model": ErrorResponse},
    },
    tags=["Text Generation Inference"],
)
def compat_generate(
    body: CompatGenerateRequest,
) -> Union[GenerateResponse | StreamResponse, ErrorResponse]:
    """
    Generate tokens if `stream == false` or a stream of token if `stream == true`
    """
    logger.info("POST /compat_generate")
    logger.debug(f"Request body: {body}")
    return GenerateResponse(generated_text="test compat generate response")


@app.post(
    "/chat_tokenize",
    response_model=ChatTokenizeResponse,
    responses={"404": {"model": ErrorResponse}},
    tags=["Text Generation Inference"],
)
def get_chat_tokenize(body: ChatRequest) -> ChatTokenizeResponse | ErrorResponse:
    """
    Template and tokenize ChatRequest
    """
    logger.info("POST /chat_tokenize")
    logger.debug(f"Request body: {body}")
    return ChatTokenizeResponse(templated_text="\n".join(body.messages), tokenize_response="sample text")


@app.post(
    "/generate",
    response_model=GenerateResponse,
    responses={
        "422": {"model": ErrorResponse},
        "424": {"model": ErrorResponse},
        "429": {"model": ErrorResponse},
        "500": {"model": ErrorResponse},
    },
    tags=["Text Generation Inference"],
)
async def generate(body: GenerateRequest) -> GenerateResponse | ErrorResponse:
    """
    Generate tokens
    """
    logger.info("POST /generate")
    logger.debug(f"Request body: {body}")
    rag_server = "http://mtg-ai-rag-webserver:8000"
    rag_search_response = await web_client.post(
        rag_server + "/query", json=body
    )
    rag_search_response.raise_for_status()
    data = rag_search_response.json()
    logger.debug(f"RAG search response: {data}")
    # TODO: Post to  the text generation servers
    response = GenerateResponse(generated_text=data["response"])
    return response


@app.post(
    "/generate_stream",
    response_model=StreamResponse,
    responses={
        "422": {"model": ErrorResponse},
        "424": {"model": ErrorResponse},
        "429": {"model": ErrorResponse},
        "500": {"model": ErrorResponse},
    },
    tags=["Text Generation Inference"],
)
async def generate_stream(body: GenerateRequest) -> StreamResponse | ErrorResponse:
    """
    Generate a stream of token using Server-Sent Events
    """
    logger.info("POST /generate_stream")
    logger.debug(f"Request body: {body}")
    rag_server = "http://mtg-ai-rag-webserver:8000"
    rag_search_response = await web_client.post(
        rag_server + "/query", json=body
    )
    rag_search_response.raise_for_status()
    data = rag_search_response.json()
    logger.debug(f"RAG search response: {data}")
    # TODO: Post to the text generation servers
    response = GenerateResponse(generated_text=data["response"])
    return response    


@app.get(
    "/health",
    response_model=None,
    responses={"503": {"model": ErrorResponse}},
    tags=["Text Generation Inference"],
)
def health() -> Optional[ErrorResponse]:
    """
    Health check method
    """
    logger.info("GET /health")
    return None


@app.get("/info", response_model=Info, tags=["Text Generation Inference"])
def get_model_info() -> Info:
    """
    Text Generation Inference endpoint info
    """
    logger.info("GET /info")
    return Info(
        model_name="test model name",
        model_description="test model description",
        model_version="test model version",
    )

@app.post(
    "/invocations",
    response_model=SagemakerResponse,
    responses={
        "422": {"model": ErrorResponse},
        "424": {"model": ErrorResponse},
        "429": {"model": ErrorResponse},
        "500": {"model": ErrorResponse},
    },
    tags=["Text Generation Inference"],
)
def sagemaker_compatibility(
    body: SagemakerRequest,
) -> SagemakerResponse | ErrorResponse:
    """
    Generate tokens from Sagemaker request
    """
    logger.info("POST /invocations")
    logger.debug(f"Request body: {body}")
    pass


@app.get("/metrics", response_model=str, tags=["Text Generation Inference"])
def metrics() -> str:
    """
    Prometheus metrics scrape endpoint
    """
    logger.info("GET /metrics")
    return "test metrics response"


@app.post(
    "/tokenize",
    response_model=TokenizeResponse,
    responses={"404": {"model": ErrorResponse}},
    tags=["Text Generation Inference"],
)
def tokenize(body: GenerateRequest) -> TokenizeResponse | ErrorResponse:
    """
    Tokenize inputs
    """
    logger.info("POST /tokenize")
    logger.debug(f"Request body: {body}")
    return TokenizeResponse(tokenized_text="test tokenize response")

async def query_rag(request: ChatCompletionInput):
    logger.debug("Getting RAG data")
    rag_server = "http://mtg-ai-rag-webserver:8000"
    rag_request = {
        "query": asdict(request.messages[-1]),
        "top_k": 5,
        "filters": None,
    }
    logger.debug(f"RAG request: {rag_request}")
    rag_search_response = await web_client.post(
        rag_server + "/query", json=rag_request
    )
    rag_search_response.raise_for_status()
    data = rag_search_response.json()
    logger.debug(f"RAG search response: {data}")


def query_ai(request: ChatCompletionInput) -> Union[dict, Iterable[dict]]:
    response: Union[ChatCompletionOutput | Iterable[ChatCompletionStreamOutput]] = inference_client.chat_completion(
        messages=request.messages,
        model=request.model,
        temperature=request.temperature,
        top_p=request.top_p,
        n=request.n,
        stream=request.stream,
        stop=request.stop,
        max_tokens=request.max_tokens,
        presence_penalty=request.presence_penalty,
        frequency_penalty=request.frequency_penalty,
        logit_bias=request.logit_bias,
        logprobs=request.logprobs,
        response_format=request.response_format,
        seed=request.seed,
        stream_options=request.stream_options,
        tool_choice=request.tool_choice,
        tool_prompt=request.tool_prompt,
        tools=request.tools,
        top_logprobs=request.top_logprobs,
    )
    if isinstance(response, Iterable):
        for r in response:
            logger.info(f"AI STREAM RESPONSE: {type(r)} {r}")
            yield asdict(r)
    else:
        logger.info(f"AI Response: {type(response)} {response}")
        yield asdict(response)

@app.post(
    "/v1/chat/completions",
    # response_model=ChatCompletionOutput,
    # responses={
    #     "422": {"model": ErrorResponse},
    #     "424": {"model": ErrorResponse},
    #     "429": {"model": ErrorResponse},
    #     "500": {"model": ErrorResponse},
    # },  
    tags=["Text Generation Inference"],
)
def chat_completions(request: ChatCompletionInput):# -> ChatCompletionOutput | ChatCompletionStreamOutput | ErrorResponse:
    """
    Generate chat responses
    """
    logger.debug(f"messages: {request.messages}")
    response = query_ai(request)
    yield from response




@app.post(
    "/v1/completions",
    response_model=CompletionFinal,
    responses={
        "422": {"model": ErrorResponse},
        "424": {"model": ErrorResponse},
        "429": {"model": ErrorResponse},
        "500": {"model": ErrorResponse},
    },
    tags=["Text Generation Inference"],
)
def completions(body: ChatCompletionInput) -> ChatCompletionOutput | ErrorResponse:
    """
    Generate tokens
    """
    logger.info("POST /v1/completions")
    logger.debug(f"Request body: {body}")
    return CompletionFinal(completed_text="test completions response")


@app.get(
    "/v1/models",
    response_model=ModelInfo,
    responses={"404": {"model": ErrorResponse}},
    tags=["Text Generation Inference"],
)
def openai_get_model_info() -> ModelInfo | ErrorResponse:
    """
    Get model info
    """
    logger.info("GET /v1/models")
    return ModelInfo(model_name="test model name", model_description="test model description")
