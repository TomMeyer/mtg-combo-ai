# generated by fastapi-codegen:
#   filename:  openapi.json
#   timestamp: 2024-12-13T23:50:23+00:00

from __future__ import annotations

from typing import Optional, Union

import httpx
from fastapi import FastAPI

from mtg_ai.ai.rag import MTGRAGSearchSystem
from mtg_ai.cards.database import MTGDatabase
from mtg_ai_webserver.models import (
    ChatCompletion,
    ChatRequest,
    ChatTokenizeResponse,
    CompatGenerateRequest,
    CompletionFinal,
    CompletionRequest,
    ErrorResponse,
    FieldDatamodelCodeGeneratorRootSpecialPostResponse,
    GenerateRequest,
    GenerateResponse,
    Info,
    ModelInfo,
    SagemakerRequest,
    SagemakerResponse,
    StreamResponse,
    TokenizeResponse,
)

app = FastAPI(
    title="Text Generation Inference",
    description="Text Generation Webserver",
    contact={"name": "Olivier Dehaene"},
    license={
        "name": "Apache 2.0",
        "url": "https://www.apache.org/licenses/LICENSE-2.0",
    },
    version="3.0.1-dev0",
)

web_client = httpx.AsyncClient()

rag_embedding_model_name: str = "sentence-transformers/all-MiniLM-L6-v2"
card_database = MTGDatabase()
rag_service = MTGRAGSearchSystem(
    database=card_database, embedding_model=rag_embedding_model_name
)


@app.post(
    "/",
    response_model=FieldDatamodelCodeGeneratorRootSpecialPostResponse,
    responses={
        "422": {"model": ErrorResponse},
        "424": {"model": ErrorResponse},
        "429": {"model": ErrorResponse},
        "500": {"model": ErrorResponse},
    },
    tags=["Text Generation Inference"],
)
def compat_generate(
    body: CompatGenerateRequest,
) -> Union[GenerateResponse | StreamResponse, ErrorResponse]:
    """
    Generate tokens if `stream == false` or a stream of token if `stream == true`
    """
    return GenerateResponse(generated_text="sample text")


@app.post(
    "/chat_tokenize",
    response_model=ChatTokenizeResponse,
    responses={"404": {"model": ErrorResponse}},
    tags=["Text Generation Inference"],
)
def get_chat_tokenize(body: ChatRequest) -> ChatTokenizeResponse | ErrorResponse:
    """
    Template and tokenize ChatRequest
    """
    pass


@app.post(
    "/generate",
    response_model=GenerateResponse,
    responses={
        "422": {"model": ErrorResponse},
        "424": {"model": ErrorResponse},
        "429": {"model": ErrorResponse},
        "500": {"model": ErrorResponse},
    },
    tags=["Text Generation Inference"],
)
def generate(body: GenerateRequest) -> GenerateResponse | ErrorResponse:
    """
    Generate tokens
    """
    rag_search_data = rag_service.search(body.inputs, top_k=10)
    formatted_rag_data = "\n".join([result.format() for result in rag_search_data])
    response = GenerateResponse(generated_text=formatted_rag_data)
    return response


@app.post(
    "/generate_stream",
    response_model=StreamResponse,
    responses={
        "422": {"model": ErrorResponse},
        "424": {"model": ErrorResponse},
        "429": {"model": ErrorResponse},
        "500": {"model": ErrorResponse},
    },
    tags=["Text Generation Inference"],
)
def generate_stream(body: GenerateRequest) -> StreamResponse | ErrorResponse:
    """
    Generate a stream of token using Server-Sent Events
    """
    pass


@app.get(
    "/health",
    response_model=None,
    responses={"503": {"model": ErrorResponse}},
    tags=["Text Generation Inference"],
)
def health() -> Optional[ErrorResponse]:
    """
    Health check method
    """
    pass


@app.get("/info", response_model=Info, tags=["Text Generation Inference"])
def get_model_info() -> Info:
    """
    Text Generation Inference endpoint info
    """
    pass


@app.post(
    "/invocations",
    response_model=SagemakerResponse,
    responses={
        "422": {"model": ErrorResponse},
        "424": {"model": ErrorResponse},
        "429": {"model": ErrorResponse},
        "500": {"model": ErrorResponse},
    },
    tags=["Text Generation Inference"],
)
def sagemaker_compatibility(
    body: SagemakerRequest,
) -> SagemakerResponse | ErrorResponse:
    """
    Generate tokens from Sagemaker request
    """
    pass


@app.get("/metrics", response_model=str, tags=["Text Generation Inference"])
def metrics() -> str:
    """
    Prometheus metrics scrape endpoint
    """
    pass


@app.post(
    "/tokenize",
    response_model=TokenizeResponse,
    responses={"404": {"model": ErrorResponse}},
    tags=["Text Generation Inference"],
)
def tokenize(body: GenerateRequest) -> TokenizeResponse | ErrorResponse:
    """
    Tokenize inputs
    """
    pass


@app.post(
    "/v1/chat/completions",
    response_model=ChatCompletion,
    responses={
        "422": {"model": ErrorResponse},
        "424": {"model": ErrorResponse},
        "429": {"model": ErrorResponse},
        "500": {"model": ErrorResponse},
    },
    tags=["Text Generation Inference"],
)
def chat_completions(body: ChatRequest) -> ChatCompletion | ErrorResponse:
    """
    Generate tokens
    """
    pass


@app.post(
    "/v1/completions",
    response_model=CompletionFinal,
    responses={
        "422": {"model": ErrorResponse},
        "424": {"model": ErrorResponse},
        "429": {"model": ErrorResponse},
        "500": {"model": ErrorResponse},
    },
    tags=["Text Generation Inference"],
)
def completions(body: CompletionRequest) -> CompletionFinal | ErrorResponse:
    """
    Generate tokens
    """
    pass


@app.get(
    "/v1/models",
    response_model=ModelInfo,
    responses={"404": {"model": ErrorResponse}},
    tags=["Text Generation Inference"],
)
def openai_get_model_info() -> ModelInfo | ErrorResponse:
    """
    Get model info
    """
    pass
