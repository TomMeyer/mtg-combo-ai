services:
  text-generation-inference:
    image: ghcr.io/huggingface/text-generation-inference:latest
    # deploy:
      # resources:
      #   reservations:
      #     devices:
      #       - driver: nvidia
      #         count: all
      #         capabilities: [gpu]
    shm_size: '1gb'
    ports:
      - "9000:9000"
    networks:
      - backend
    environment:
      - MODEL_ID=meta-llama/Llama-3.2-1B-Instruct
      # - LORA_ADAPTERS=${adapter}
      - HF_TOKEN=${HUGGINGFACE_TOKEN}
      - QUANTIZE=bitsandbytes-nf4

  mtg_ai_rag_webserver:
    image: mtg-ai-rag-webserver:latest
    ports:
      - "8889:8889"
    networks:
      - backend
    depends_on:
      - text-generation-inference

  mtg_ai_webserver:
    image: mtg-ai-webserver:latest
    ports:
      - "8080:8080"
    networks:
      - backend
    depends_on:
      - text-generation-inference

  chat-ui:
    image: ghcr.io/huggingface/chat-ui-db:latest
    ports:
      - "3000:3000"
    networks:
      - backend
    env_file:
      - path: ./chat-ui-env.local
    # environment:
    volumes:
      - chat-ui:/data

  nginx:
    image: nginx:latest
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - text-generation-inference
      - mtg_ai_webserver
      - mtg_ai_rag_webserver
    networks:
      - backend
      - frontend

networks:
  backend:
    internal: true
  
  frontend:
    internal: false
  

volumes:
  chat-ui:


