services:
  text-generation-inference:
    image: ghcr.io/huggingface/text-generation-inference:latest
    # deploy:
      # resources:
      #   reservations:
      #     devices:
      #       - driver: nvidia
      #         count: all
      #         capabilities: [gpu]
    shm_size: '1gb'
    networks:
      - backend
    environment:
      - MODEL_ID=meta-llama/Llama-3.2-1B-Instruct
      # - LORA_ADAPTERS=${adapter}
      - HF_TOKEN=${HUGGINGFACE_TOKEN}
      - PORT=8080
      # - QUANTIZE=bitsandbytes-nf4
    volumes:
      - llm-model:/data

  mtg-ai-rag-webserver:
    image: mtg-ai-rag-webserver:latest
    networks:
      - backend
    volumes:
      - ./src/mtg_ai_rag_webserver:/home/appuser/mtg-ai/src/mtg_ai_rag_webserver
    depends_on:
      - text-generation-inference

  mtg-ai-webserver:
    image: mtg-ai-webserver:latest
    networks:
      - backend
    depends_on:
      - text-generation-inference
      - mtg-ai-rag-webserver
    volumes:
      - ./src/mtg_ai_webserver:/home/appuser/mtg-ai/src/mtg_ai_webserver

  chat-ui:
    image: ghcr.io/huggingface/chat-ui-db:latest
    networks:
      - backend
    # env_file:
      # - path: ./chat-ui-env.local
    volumes:
      - chat-ui:/data
      - ./chat-ui-env.local:/app/.env.local

  nginx:
    image: nginx:latest
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - text-generation-inference
      - mtg-ai-webserver
      - mtg-ai-rag-webserver
      - chat-ui
    networks:
      - backend
      - frontend

networks:
  backend:
    internal: false
  
  frontend:
    internal: false
  

volumes:
  chat-ui:
  llm-model:


